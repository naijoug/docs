---
title: 音视频
icon: hashtag

index: true

---

<!-- more -->

## reference

- [RTC-Developer](https://github.com/RTC-Developer)
- [2021.03.02 在线教室 iOS 端声音问题综合解决方案](https://juejin.cn/post/6934987607088726053#heading-12)

<!-- 编码格式 -->

- [H264 vs H265](https://mattgadient.com/x264-vs-x265-vs-vp8-vs-vp9-examples/)
- [2019.07.05 iOS视频压缩笔记](https://www.jianshu.com/p/4f69c22c6dce)
- [File Size Issue with HEVC Encoder on iOS 11](https://medium.com/@jpetrichsr/file-size-issue-with-hevc-encoder-on-ios-11-f50104ace54f)

<!-- 边下边播 -->

- [2021.04.21 开发播放器框架之边下边播边存方案](https://github.com/yangKJ/KJPlayerDemo/wiki/%E5%BC%80%E5%8F%91%E6%92%AD%E6%94%BE%E5%99%A8%E6%A1%86%E6%9E%B6%E4%B9%8B%E8%BE%B9%E4%B8%8B%E8%BE%B9%E6%92%AD%E8%BE%B9%E5%AD%98%E6%96%B9%E6%A1%88)
- [2020.02.05 Understanding AVAssetResourceLoaderDelegate](https://github.com/2ZGroupSolutionsArticles/Article_EZ002)
- [2019.12.03 iOS AVPlayer 视频缓存的设计与实现](http://chuquan.me/2019/12/03/ios-avplayer-support-cache/)
- [2019.04.23 AVPlayer 边下边播与最佳实践](https://zltunes.github.io/2019/04/23/avplayer-best-practice/)
- [iOS 实现在线视频边下边播](https://github.com/Kaibin/YYVideoPlayer)
- [2018.08.31 iOS短视频播放缓存之道](https://segmentfault.com/a/1190000016228456)
    > [ShortMediaCache](https://github.com/dangercheng/ShortMediaCache)
- [2018.02.28 AVPlayer 初体验之边下边播与视频缓存](http://xferris.cn/avplayer_cache/)
- [2017.06.13 Swift 封装一个视频播放器 VGPlayer](https://www.jianshu.com/p/1680978e1a7e)
    > [VGPlayer](https://github.com/VeinGuo/VGPlayer)
- [2017.03.31 可能是目前最好的 `AVPlayer` 音视频缓存方案](https://mp.weixin.qq.com/s/v1sw_Sb8oKeZ8sWyjBUXGA?#%23)
    > [VIMediaCache](https://github.com/vitoziv/VIMediaCache)
- [2016.09.03 Implementing AVAssetResourceLoaderDelegate: a How-To Guide](https://jaredsinclair.com/2016/09/03/implementing-avassetresourceload.html)
- [2016.07.12 CachingPlayerItem](https://github.com/neekeetab/CachingPlayerItem)
- [2016.05.24 iOS音频播放 (九)：边播边缓存](https://msching.github.io/blog/2016/05/24/audio-in-ios-9/)

------

## 框架

### ❓MetalKit

::: details 💡

> `MetalKit`：Apple开发的一个专门用于 `iOS` 和 `macOS` 设备上支持 `Metal` 编程接口的应用程序框架。提供的为了提升图形和计算任务性能的硬件加速 `API`。与 `OpenGL` 和 `OpenGL ES` 相比，`Metal` 提供了更接近硬件的 `API`，因此可以提供更高的性能和更低的延迟。

主要功能：

  - 图形渲染：可以用于绘制 `2D` 和 `3D` 图形，实现图形渲染的任务，如游戏图形、科学计算可视化等。
  
  - 加载和处理纹理：可以用于加载图像文件到 `Metal` 纹理，以及纹理信息的查询和使用。

  - 加载 `3D` 模型：可以加载 `3D` 模型数据并将其转换为 `Metal` 可理解的数据格式。

  - 显示管理：提供了用于在 `iOS` 和 `macOS` 上创建和管理 `Metal` 绘图层的工具。

  - 封装常见 `Metal` 数据结构：例如矩阵和向量等，方便用于图形和计算任务。

  - 性能调试工具：包含了用于监控和优化 `Metal` 应用性能的工具。

:::

### ❓GPUImage

::: details 💡

> `GPUImage` 是开源的图像和视频处理框架，对视频处理、人像美颜、图像处理、滤镜处理等领域有广泛应用。

  - 功能强大：`GPUImage` 中内置了许多滤镜，用户可以对视频和图片进行各种各样的应用，也可以自己实现自定义的滤镜。

  - 高性能：`GPUImage` 是基于 `OpenGL ES` 构建的，能够充分利用设备的 `GPU` 处理大量计算任务，比使用 `CPU` 处理效率更高，对实时视频处理场景非常有利。

  - 易用性：`GPUImage` 的 `API` 设计封装得相当完善，使用起来十分方便，同时文档全面，上手容易。

  - 富有扩展性：`GPUImage` 也允许用户自定义滤镜，用户可以实现自己的设计，具有很高的灵活性。

  - 跨平台：`GPUImage` 既有适用于 `iOS` 的版本，也有适用于 `Android` 的版本，这大大方便了在多平台开发过程中的一致性。

:::

### ❓OpenGL vs OpenGL ES

::: details 💡

> `OpenGL` (Open Graphics Library) 和 `OpenGL ES` (OpenGL for Embedded Systems) 都是用于渲染二维和三维图形的跨平台 `API` 的，由 `Khronos Group` 制定和维护。这种能力使得 `OpenGL` 成为虚拟现实、游戏、三维绘图等领域的标准规范。

主要区别：
  > `OpenGL ES` 是专门为嵌入式设备（例如手机或者嵌入式系统）设计的，相比 `OpenGL`，`OpenGL ES` 删除了一些不常用或者不适用于嵌入式系统的特性以减少系统资源的消耗，从而在保证性能的基础上达到对系统资源的有效利用。

  - 功能和复杂性：`OpenGL` 更为全面和复杂，有更多的函数和选项可供使用。而 `OpenGL ES` 是一个更精简的版本，去掉了一些在嵌入式设备上不常需要或者性能消耗较大的功能，如立方体贴图、多纹理等。

  - 系统资源占用：由于 `OpenGL ES` 设计得更为精简，因此它对系统内存和处理器的需求通常较 `OpenGL` 更低。
    
  - 平台支持：`OpenGL` 广泛用于桌面系统和高性能计算设备，如 `Windows`，`macOS` 和 `Linux` 系统。而 `OpenGL ES` 则主要用于嵌入式设备和移动设备，如 `Android`、`iOS` 和某些游戏机。

  - 版本更新：`OpenGL ES` 的版本更新通常会跟随 `OpenGL` 的更新，但在某些特性上可能会有异步。例如，`OpenGL ES 3.0` 是在 `OpenGL 4.2` 之后推出的，但并未包含 `OpenGL 4.2` 的全部新特性。


`OpenGL` 处理流程：
  > 注意：`OpenGL` 是一个状态机，它会保存设置的所有状态，直到将其改变为止。因此，在绘制过程中，需要处理 `OpenGL` 状态以防止从前的设置影响到后面的渲染。
  
  - **顶点处理**：原始顶点坐标会通过一系列的操作变换到屏幕坐标系。这些操作包括变换（旋转、缩放、平移）、裁剪（去除不在视野内的顶点）等。可以用自定义的顶点着色器（`Vertex Shader`）程序进行处理的。

  - **图元装配**：在所有的顶点被处理后 `OpenGL` 开始将这些顶点装配成图形基元（如点、线、面）。

  - **光栅化**：图元在屏幕上被划分为一系列的片元（像素），这个过程就叫做光栅化。
  
  - **片元处理**：在这个阶段，片元着色器（`Fragment Shader`）会处理每个片元，比如对片元添加颜色、纹理或执行更复杂的效果。
  
  - **深度测试和混合**：`OpenGL` 对这些片元进行深度测试，判断哪些片元是真正可见的。上面的所有阶段都只用来生成一堆片元，以及每个片元对应的深度值。最后如果开启混合，还会根据源与目标的 `alpha` 值来计算最后的颜色值。

:::

### ❓AVFoundation vs GPUImage vs OpenGL ES vs MetalKit

::: details 💡

  - `AVFoundation`：`Apple` 开发的一个用于处理音频和视频的框架。它提供了一套丰富的 `API` 和工具，可以用于播放和创建音频和视频，也可以用于处理实时的音频视频流和管理设备的摄像头和麦克风。除此之外，它还支持视频编辑，视频捕获，视频解码等功能。

  - `GPUImage`：一个开源的图像和视频处理库，它利用 `GPU`（图形处理器）来处理图像和视频，因此在处理大量数据时能实现非常高的效率。它提供了大量的内置滤镜和效果，也可以自定义滤镜。

  - `OpenGL ES`：一个用于嵌入式系统的 `2D` 和 `3D` 图形API（如手机，平板电脑等）。由 `Khronos Group` 开发，并且是 `OpenGL` 的子集。它被广泛用于游戏和其他需要高效渲染 `3D` 图形的应用。

  - `MetalKit`：`Apple` 提供的一个用于在 `iOS` 设备上更高效地直接访问 `GPU` 的框架。它被设计用于创建对渲染性能有高要求的图形应用，例如游戏。比起 `OpenGL ES` 提供了更低的 `API` 抽象层，这让开发者能够更直接地控制硬件，因此能够获得更高的性能。

:::

### ❓FFmpeg

::: details 💡

> `FFmpeg`是一个开源的音视频处理软件，用于处理音频、视频、数据等多媒体内容。它提供了录制、转换和流化音视频的完整解决方案。

一套可以用来解码、编码、传输多媒体数据的库：

  - `libavcodec`：用于编码/解码的库，支持众多的编码解码器。
  - `libavformat`：用于多媒体数据封装和解封装的库，支持众多的封装和解封装格式，如 `MP4、FLV、MOV` 等。
  - `libavutil`：包含许多工具函数的库，为其他 `FFmpeg` 的库或应用程序提供使用。
  - `libavfilter`：视频和音频过滤器的库，可用于图像的缩放，裁剪，颜色转换等。
  - `libswscale`：处理图像缩放和像素格式转换的库。
  - `libswresample`：处理音频重采样、重混合和格式转换的库。

一些对音视频数据处理的命令行工具：

  - `ffmpeg`：可以对多媒体数据进行转码，截取视频片段，提取音频，进行视频剪辑等。
  - `ffplay`：可以播放音频和视频。
  - `ffprobe`：可以查看多媒体文件的详细信息。
    
    ```shell
    # 将一个 mp4 视频转码为 avi 格式
    ffmpeg -i input.mp4 output.avi
    ```

:::


### ❓OpenCV

::: details 💡

> `OpenCV`(Open Source Computer Vision)：一个开源的计算机视觉库。包含了大量的计算机视觉、数字图像处理以及一些通用的科学计算的相关的函数库。支持多种语言，如 `C++、Python、Java` 等，同时也支持 `Windows、Linux、macOS、iOS、Android` 等多种操作系统。

核心模块：

  - 图像处理：包括滤波、锐化、色彩空间转换、直方图均衡化等。
  - 图像分割：如边缘检测和切割算法。
  - 视频分析：比如光流法、背景/前景分割等。
  - 对象识别：比如面部和眼睛识别。
  - 机器学习：支持 `KMeans`、`SVM`、神经网络等算法。

```py
# 使用 OpenCV 进行面部和眼睛识别

import cv2
# 加载 Haar 级联分类器的 XML 文件
face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')
eye_cascade = cv2.CascadeClassifier('haarcascade_eye.xml')
# 读取图片
img = cv2.imread('your_face_image.jpg')
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
# 检测图像中的面部和眼睛
faces = face_cascade.detectMultiScale(gray, 1.3, 5)
for (x,y,w,h) in faces:
    img = cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)
    roi_gray = gray[y:y+h, x:x+w]
    roi_color = img[y:y+h, x:x+w]
    eyes = eye_cascade.detectMultiScale(roi_gray)
    for (ex,ey,ew,eh) in eyes:
        cv2.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,255,0),2)

cv2.imshow('img',img)
cv2.waitKey(0)
cv2.destroyAllWindows()
```

:::

------

## 编码

### ❓硬编码 vs 软编码

::: details 💡

硬编码（硬件编码）：通过硬件来进行媒体数据的编码和解码。
  > 这些硬件包括用于编码解码（`Codec`）的专门芯片或者 `GPU` 等。
  
  - 优点：硬编码通常比软编码快，因为硬件编码器是专门设计用于编码/解码的，它们的算法优化级别比软件更高；消耗的 `CPU` 资源较少，因为大部分工作都在特定的硬件上执行。
  - 缺点：灵活性差，依赖于硬件的能力，针对个别视频编码/解码技术可能没有等效的硬件支持。

软编码（软件编码）：在 `CPU` 上进行编码和解码的过程。
  > 软编码不依赖于特定的硬件，而是依赖 `CPU` 和编码/解码软件（例如`FFmpeg`，`x264`等）。
  
  - 优势：灵活性高，它可以很容易地优化和升级，以适应新的编码标准和格式。而且通常软编码在视频质量和压缩效率方面表现更好。
  - 缺点：耗费的 `CPU` 资源较多，可能导致系统反应慢或者其他任务变慢。

应用：选择硬编码和软编码需要根据具体的应用场景和需求决定。

  - 如果是在电池供电的设备上进行实时视频流处理，可能更倾向于使用硬编码，因为它对电池使用的影响更小。
  - 如果需要最高的视频质量和最小的文件大小，可能会选择软编码。

:::

### ❓FBO、H264、H265

::: details 💡

  - `FBO`(Frame Buffer Object): 帧缓冲对象，是 `OpenGL` 的一个扩展，用来管理帧缓冲区。
    > 帧缓冲区保存了渲染操作的中间或最终结果。使用 `FBO` 可以把渲染结果保存到一个纹理对象中，后续可以直接用来进行渲染。`FBO` 让开发者更方便的在 `GPU` 上进行渲染，同样也可以用来处理图片和视频。

  - `H264`: 一种视频编解码标准，也叫 `AVC`（Advanced Video Coding，高级视频编码）。
    > `H264` 提供了一种高质量、有效的视频压缩方式，它比之前的编解码标准如 `MPEG-2` 和 `MPEG-4` 要好很多。`H264` 编解码器在采集和播放视频时会广泛用到，它是目前互联网上的主流视频编码格式。

  - `H265`: `H264` 的后继者，也叫 `HEVC`（High Efficiency Video Coding，高效视频编码）。
    > `H265` 提供了更高的数据压缩率，它可以在保持相同视频质量的同时，将文件大小压缩到 `H264` 的一半左右，或者用相同的文件大小提供更好的视频质量。`H265` 的复杂度也更高，所以需要更多的 `CPU` 和 `GPU` 资源来编解码。

:::

### ❓计算 PCM 时长

::: details 💡

> `PCM`（Pulse Code Modulation）是一种数字化音频的方式。

`PCM` 数据时长计算公式：`时间（秒）= 数据大小（字节） / (采样率 * 位深度/8 * 通道数)`

  - 数据大小：`PCM` 文件的大小(字节）
  - 采样率：录音设备在一秒钟内对声音信号的采样次数，通常是44.1kHz，48kHz或96kHz。
  - 位深度：录音设备在对每次采样的声音信号进行模拟转数字的处理过程中，能够记录的声音信号的精度，通常是16位或24位。
  - 通道数：音频的声道数，如单声道（`Mono`）为1，立体声（`Stereo`）为2。

🌰 有一个 5MB 的 `PCM` 文件，采样率为 44.1kHz，采样位深度为 16 位，为立体声音频。
  > `时间 = 5 * 1024 * 1024 (字节) / (44.1 * 1024 * 16/8 * 2) ≈ 30秒`

:::

------

## 压缩

### ❓`JPEG` 的压缩过程

::: details 💡

> `JPEG`（`Joint Photographic Experts Group`）是一种常用的图像压缩标准，主要应用于数字摄影和网络图像。

压缩过程：

  - 颜色空间转换：将图像的颜色空间从 `RGB` 转换到 `YCbCr`。`Y` 是指亮度，`Cb` 和 `Cr` 是指色度，这样做的目的是为了减少人眼对色彩的敏感度。

  - 分块：将图像划分为 8x8 的小块，每一个 8x8 的小块进行独立的处理。

  - 离散余弦变换（`DCT`）：对每个 8x8 的像素块应用离散余弦变换，将其中的像素值从空间域转换到频域。在频域中，图片信息主要集中在低频部分，高频部分可以进行剔除以达到压缩的目的。

  - 量化：通过量化表对 `DCT` 变换后的系数进行量化，这个过程是有损的，通常会丢失一些高频信息。

  - 重新排序：将量化后的系数按照特定的顺序重排，通常是沿着对角线的方向。

  - 熵编码：使用哈夫曼编码对重排后的系数进行编码，该过程是无损的，用于进一步减小文件大小。

注：`JPEG` 压缩算法的最后两步使得压缩过程不可逆，也就是无法恢复到原始的图像。尽管如此，`JPEG` 算法仍被广泛使用，因为它能够在压缩图像文件大小的同时保留足够的视觉信息。

:::

### ❓主流的视频压缩协议

::: details 💡

  - `H.264/AVC`：高级视频编码（`Advanced Video Coding`），也被誉为是目前应用最广泛的压缩标准，它被广泛应用于网络视频、视频会议、数字电视、蓝光光盘等领域。
  - `H.265/HEVC`：高效视频编码（`High Efficiency Video Coding`），是 `H.264` 的后续版本，对比 `H.264`，它在保证相同画质的情况下，可达到更高的压缩效率。
  - `AV1`：开源、免版权费的视频编码格式，由开放媒体联盟 `AOMedia` 开发，提供比 `H.265` 高出 20% 的压缩效率。

  - `VP8`：是`Google`发布的开源视频压缩格式，由于其高效的压缩算法，被广泛应用于 `Web` 视频。
  - `VP9`：由 `Google` 开发的一个开源、免版权费的视频压缩标准，是 `VP8` 的改进版，主要用于网络视频流和 `WebRTC` 领域。

  - `MPEG-2`：主要用于数字视频广播和 `DVD` 视频的标准，压缩效率较低，但在兼容性和质量方面有很好的表现。
  - `MPEG-4`：是一种用于时间基本媒体（如音频、视频）的国际编码标准，包含多种编码方法（比如`DivX`，`Xvid`等）。

:::

### ❓视频压缩的原理

::: details 💡

> 视频压缩是一种技术，用于减少数据量以便在网络上流畅地传输视频或在存储设备上节省空间。

基本原理：

  - 空间压缩：也被称为帧内编码，主要针对视频帧内部进行压缩。比如通过转换颜色空间，量化，变换编码等方法对帧内信息进行压缩。在此过程中，主要利用了人类视觉系统对某些信息（例如高频信息，色度信息）的不敏感性，来移除这些我们看不到或者看不清的信息。

  - 时间压缩：也被称为帧间编码，主要针对连续帧间进行压缩。因为在一段连续的视频帧中，存在大量的冗余信息。比如在前后帧中，背景和主体的大部分区域并未发生变动。此时，可以通过预测的方法，找出当前帧与前后帧的差异，而只存储这些差异信息即可。

  - 熵编码：在前两步压缩后，得到的数据中，一些值出现得非常频繁，而一些则很少出现。针对这种情况，我们可以赋予这些频繁出现的值较短的编码，而赋予较少出现的值较长的编码，以此降低数据的平均长度，获得更高的压缩效果。这就是熵编码的原理，常用的熵编码方法有哈夫曼编码和算术编码等。
    
:::

------

## 滤镜

### ❓滤镜实现原理

::: details 💡

> 滤镜的实现原理：是对图像上的每一个像素进行处理以达到预期的效果。处理的方式取决于滤镜的类型，例如：调整明暗、颜色、对比度等，以及更复杂的操作如模糊等。

主要步骤：

  - 对图片的每一个像素进行迭代。
  - 在每个像素上应用一个函数或者算法。最简单的例子可能就是调整像素的 `RGB` 值来改变图片颜色。
  - 将处理后的像素值重新组合成图片。

```swift
// 使用 Core Image 在中创建滤镜的
let ciImage = CIImage(image: image)

// 创建滤镜：内置的 "CIColorMonochrome" 滤镜 - 将图片转变为单色图像。
let filter = CIFilter(name: "CIColorMonochrome")
// 设置滤镜的输入
filter?.setValue(ciImage, forKey: kCIInputImageKey)
// 获取滤镜的输出
let result = filter?.value(forKey: kCIOutputImageKey) as? CIImage
// 创建从核心图像转换成 UIImage 的 CIContext 
let ciContext = CIContext(options: nil)
// 将图片数据导出为 CGImage，然后转化为 UIImage
if let output = ciContext.createCGImage(result!, from: (result?.extent)!) {
    let resultImage = UIImage(cgImage: output)
}
```

:::

### ❓如何实现分割滤镜

::: details 💡

> 分割滤镜的基本思路：对图片上的每个像素进行迭代，并在每个像素上根据其位置或某种规则应用不同的滤镜效果。

```swift
// 一张图片一半应用黑白效果，另一半保持原样

// 原始图片
let originalCIImage = CIImage(image: originalUIImage)!

// 创建一个黑白滤镜
let bwFilter = CIFilter(name: "CIColorMonochrome")!
bwFilter.setValue(originalCIImage, forKey: kCIInputImageKey)
let bwCIImage = bwFilter.outputImage!

let totalWidth = originalCIImage.extent.width
let slicePoint = totalWidth / 2

// 创建一个 CIBlendWithMask 滤镜 - 根据掩码图像（在模板图像中）组合两个图像
let blendFilter = CIFilter(name: "CIBlendWithMask")!
blendFilter.setValue(bwCIImage, forKey: kCIInputImageKey)
blendFilter.setValue(originalCIImage, forKey: kCIInputBackgroundImageKey)

let maskImage = CIImage(color: CIColor(red: 1, green: 1, blue: 1))
let mask = maskImage.cropped(to: CGRect(
     x: 0,
     y: 0,
     width: slicePoint,
     height: originalCIImage.extent.height))

blendFilter.setValue(mask, forKey: kCIInputMaskImageKey)

let outputImage = blendFilter.outputImage!
let finalImage = UIImage(ciImage: outputImage)
```

:::

### ❓美颜瘦脸等功能怎么实现的

::: details 💡

> 美颜瘦脸等功能的实现是通过图像处理和计算机视觉技术来实现的。

实现步骤：

  - 人脸检测：首先需要定位到图片中人脸的位置，这可以通过包括 `Haar` 特征、`HOG` 特征结合支持向量机(`SVM`)以及深度学习等技术来实现。

  - 特征点定位：定位到人脸的基础上，还需要定位出人脸上的关键特征点，如眼睛、鼻子、嘴巴、脸颊等位置。这同样可以通过机器学习或者深度学习的算法来实现。

  - 图像滤波美颜：对图像进行滤波处理，如均值滤波、高斯滤波、双边滤波等，可以去除皮肤上的瑕疵、皱纹，使得皮肤看起来更加光滑清晰。

  - 色彩调整：通过调整图像的亮度、对比度、饱和度、色调等，可以使得皮肤看起来更加健康有活力。

  - 美型瘦脸：通过图像的仿射变换或者几何变换，可以改变脸部或者身体的形状，达到美型或者瘦身的效果。

  - 脸形调整：可以通过找到脸部的关键点定位，再进行适当的像素移动和调整，达到瘦脸的效果。

  - 总体渲染：将以上步骤合并，对原始图像进行渲染，生成最终的美颜瘦脸的效果。

:::

------

## 直播

### ❓直播整体流程

::: details 💡

  - **信号采集**：使用手机、电脑或专用设备采集音视频信息。这个过程通常需要调用设备的摄像头和麦克风等硬件设备进行实时音视频录制。

  - **编码与压缩**：将采集到的音视频信号进行编码和压缩，转成可以网络传输的流媒体格式，如 `H264, H265, VP8, VP9` 等视频编码和 `AAC、MP3` 等音频编码。

  - **上传推流**：将编码后的音视频数据通过网络发送到直播服务器。这个过程需要稳定、连续的网络连接来保证信号的实时性。

  - **服务器接收与转发**：直播服务器接收到直播数据后，会进行一些必要的处理，如转码、录制、插播广告等，然后转发给观看的用户。

  - **观众拉流&解码**：观众端拉取直播流数据，然后解码播放。这个过程也需要稳定的网络环境，并且要解决视频播放的同步和时延问题。

  - **交互反馈**：观众与主播进行互动，如发弹幕、送礼物、评论等，这些都需要实时反馈到直播界面上。

:::

### ❓视频从 `采集 -> 显示 -> 保存` 整个流程

::: details 💡

  - **采集**：视频的采集通常由摄像头完成，摄像头将实时场景转换为数字信号。此过程涉及到图像采集设备的工作原理，以及光学和感光原理等内容。

  - **预处理**：采集到的数字信号进行预处理，这包括颜色空间转换，降噪处理，明暗处理等步骤，目的是提高视频的感知质量。

  - **编码**：编码阶段将预处理后的视频数据转换为特定格式，使其能够被存储或传输。编码通常需要考虑压缩的需要，用来减少视频数据的大小，包含 `H264，H265，VP8，VP9` 等编码格式。

  - **封装**：编码后的视频数据需要进行封装，封装数据包括音频流、视频流以及字幕等，常见的封装格式有 `MP4，FLV，MKV，AVI，MOV` 等。

  - **传输**：视频数据通过网络传输到服务器或者传输到其他设备。可以使用 `RTMP，HLS，HTTP-FLV` 等传输协议。

  - **解码**：接收端收到视频数据后，先解封装，然后将音频和视频数据解码回原始格式。

  - **后处理**：解码后的数据可以进行一定的后处理，如通过硬件加速提高播放性能，或者对图像进行增强处理等。

  - **显示**：将解码后的视频数据显示到屏幕上。

  - **保存**：如果需要，可以将视频数据保存到本地存储设备中。此时，视频数据通常以某种文件格式保存，可以是已经编码和封装的格式，也可以是原始的未编码格式。

:::

### ❓RTMP vs WebRTC

::: details 💡

概念：

  - `RTMP`(Real Time Messaging Protocol)：即实时消息传送协议，是为了解决音视频数据在网络上高效传输而生的一个协议。RTMP是一种设计用来进行实时信息传输的网络协议，主要用来在Flash/AIR平台和支持RTMP协议的流媒体/交互服务器之间进行音视频和数据通信。RTMP流媒体服务器有Red5、Nginx、Adobe Media Server等。主要用于直播系统。

  - `WebRTC`(Web Real-Time Communication)：一项实时通讯技术，它允许网络应用或者网站不借助中间媒介，建立浏览器之间点对点（Peer-to-Peer）高质量视频音频或者数据通道。可以用于视频会议，文件传输等。WebRTC是开源的，且兼容性良好，不少现代的网络浏览器（如Chrome，Firefox）都原生支持WebRTC。

区别：

  - 网络传输：WebRTC支持P2P，RTMP不支持。WebRTC可以在同一局域网内打通P2P连接，传输效率高。而RTMP只能走服务器中转。

  - 实时性：WebRTC优于RTMP，一般网页直播采用WebRTC协议，手机直播采用RTMP协议。

  - 易用性：WebRTC只需要浏览器就可以使用，无需额外的插件。而Flash需要插件支持，使用相对复杂。

  - 兼容性：WebRTC只支持部分浏览器，而RTMP基本所有浏览器都可以支持。

  - 延时：WebRTC的延时较低，通常可以在毫秒级别，适合需要快速反馈的应用。而RTMP的延时较高，通常在几秒到十几秒不等。

:::

#### RTM & RTC & RTMP

::: details 💡

  - RTM (Real-Time Messaging) : 实时消息(信令)
    > 传输数据 : 消息文本或加密数据
    > 实时消息场景 : 消息发送、呼叫邀请
    
  - RTC (Real-Time Communication) : 实时通信
    > 传输数据 : 音视频数据
    > 实时通信场景 : 语音通话、视频通话、音频互动直播、视频互动直播 

  - RTMP/RTMPS (Real-Time Transmit Protocol Security) : 实时传输协议

:::

------

## 音视频

### 概念知识

#### ❓CMTime

::: details 💡

> `CMTime`：`Core Media` 框架中的一个 `C` 结构，用于在多媒体处理中表达时间。这个结构提供了比浮点数更准确更灵活的时间表达方式，它可以表示时间，时间范围和时间映射。

结构：

  - `value`: 一个 64 位整数表示时间值。
  - `timescale`: 一个 32 位整数，表示每秒中的帧数。
  - `flags`: 表明这个时间是有效的，还是不确定的，或者是正向的，还是反向的。
  - `epoch`: 它用于表述超过了标准的“每秒帧数”，一般应用中不会使用。


公式：`时间 = value / timescale (单位：秒)`

:::

#### ❓SampleBuffer vs PixelBuffer

::: details 💡

  - `CMSampleBufferRef` : `Core Media` 框架中被用来管理音频、视频、字幕等样本的集合。
    > 一个 `SampleBuffer` 包含一到多个样本以及描述这些样本的元数据，例如显示时间戳、解码时间戳、持续时间、帧速率等。`SampleBuffer` 在更高的层级管理数据，包含了解压缩和播放媒体所需的所有信息。

  - `CVPixelBufferRef` : `Core Video` 框架中被用来管理图像数据。
    > 一个 `PixelBuffer` 表示一个视频帧，它包含了视频帧的原始像素数据。`PixelBuffer` 是在更低的层级处理数据，仅仅包含了像素信息，没有时间戳、持续时间等元数据。

  > 两者区别：`SampleBuffer` 是对包含音频、视频、字幕等样本的集合的封装，包含了播放媒体所需的所有信息，`PixelBuffer` 是对视频帧的原始像素数据的封装，更关注图像层面的细节。

:::

### 音频处理

#### ❓`iOS` 中音频降燥如何处理

::: details 💡

  - 使用系统的 `AVAudioEngine` 类来进行处理
    
   ```swift
   let audioEngine = AVAudioEngine()
   let mixer = audioEngine.mainMixerNode // 混频器节点
   mixer.outputVolume = 0.5 // 设置输出音频的半音大小
   ```

  - 使用第三方库 `AudioKit` 进行噪音降低

   ```swift
   import AudioKit
    
   let noise = AKWhiteNoise() // 创建一个白噪声
   noise.amplitude = 0.2 // 调整噪声幅度
   AudioKit.output = noise
   try? AudioKit.start()
   noise.start()
   ```

  - `Novocaine` 是一种强大但简单的音频处理库。`Novocaine` 库本身并没有直接提供降噪处理的功能，但是它提供了低延迟的音频输入和输出，可以方便地将音频数据流输入到 `DSP` 代码中进行处理后再输出。
    
    ```swift
    // 声明一个 Novocaine 音频处理对象
    let audioManager = Novocaine.audioManager()
    
    // 设置音频输入回调
    audioManager.setInputBlock { (numberFrames, samples) in
        // 'samples' 这里是一个指向输入音频数据片段的指针，
        // 可以将这些数据传入降噪算法进行处理
        // 例如：yourNoiseReductionAlgorithm(samples)
    }
    
    // 开始运行
    audioManager.play()
    ```

  - `WebRTC`：由 `Google` 开发，用于音频和视频的实时通信。提供了一个强大的音频处理引擎，包括降噪、回声消除等功能。

  - `EZAudio`：一个 `iOS` 和 `macOS` 的音频框架，提供了方便的音频分析和处理功能，可以使用它的音频输入、输出和混音功能，结合一些 `DSP` 技术，如滤波器，对音频进行降噪处理。

  - `Superpowered`：一个高性能、低功耗的跨平台音频引擎。为开发者提供一系列音频相关的`API`，如解码、混音、滤波、`Echo`消除等。

:::

### 视频处理

#### ❓`iOS` 中视频合成如何处理

::: details 💡

> 使用 `AVMutableComposition` 类来合成多个视频片段

```swift
import AVFoundation

// 创建一个 AVMutableComposition 对象，它表示一个新的可变编辑环境：
let mixComposition = AVMutableComposition()
// 获取需要合成的视频资料并创建对应的 AVAsset 对象
let videoAssetA = AVAsset(url: URL(fileURLWithPath: "path/to/video_a.mp4"))
let videoAssetB = AVAsset(url: URL(fileURLWithPath: "path/to/video_b.mp4"))
// 创建 AVMutableCompositionTrack 对象并添加视频文件：
if let compositionVideoTrack = mixComposition.addMutableTrack(withMediaType: AVMediaType.video, preferredTrackID: CMPersistentTrackID(kCMPersistentTrackID_Invalid)),
    let sourceTrackA = videoAssetA.tracks(withMediaType: AVMediaType.video).first,
    let sourceTrackB = videoAssetB.tracks(withMediaType: AVMediaType.video).first {
        do {
            try compositionVideoTrack.insertTimeRange(CMTimeRangeMake(start: CMTime.zero, duration: videoAssetA.duration), of: sourceTrackA, at: CMTime.zero)
            try compositionVideoTrack.insertTimeRange(CMTimeRangeMake(start: CMTime.zero, duration: videoAssetB.duration), of: sourceTrackB, at: videoAssetA.duration)
        } catch {
            print(error)
        }
}
// 创建 AVAssetExportSession 对象，设置输出格式和路径，然后开始导出
let outputFilePath = "path/to/output.mp4"
let outputFileURL = URL(fileURLWithPath: outputFilePath)
if let exporter = AVAssetExportSession(asset: mixComposition, presetName: AVAssetExportPresetHighestQuality) {
    exporter.outputURL = outputFileURL
    exporter.outputFileType = AVFileType.mp4
    exporter.shouldOptimizeForNetworkUse = true
    exporter.exportAsynchronously {
        switch exporter.status {
        case .completed:
            print("Export complete")
        case .failed:
            print("Failed: \(exporter.error?.localizedDescription ?? "unknown error")")
        case .cancelled:
            print("Cancelled")
        default:
            print("Unknown")
        }
    }      
}
```

:::

#### ❓`iOS` 中视频如何添加水印

::: details 💡

```swift
// 1> AVMutableComposition：管理所有的音频、视频和时间线资源
let mixComposition = AVMutableComposition()
guard let videoTrack = mixComposition.addMutableTrack(withMediaType: .video, preferredTrackID: Int32(kCMPersistentTrackID_Invalid)) else { return }
    
// 2> 从原始视频资源获取轨道，并添加到 composition 中
guard let assetTrack = asset.tracks(withMediaType: .video).first else { return }
do {
    try videoTrack.insertTimeRange(CMTimeRangeMake(start: CMTime.zero, duration: asset.duration),
                                   of: assetTrack,
                                   at: CMTime.zero)
} catch {
    print("Failed to load video track")
    return
}

// 3> 创建一个 CALayer 用于显示水印并添加到 composition 的 layer 中
let size = videoTrack.naturalSize
let watermarkLayer = CATextLayer()
watermarkLayer.string = "Watermark"
watermarkLayer.foregroundColor = UIColor.red.cgColor
watermarkLayer.fontSize = 40
watermarkLayer.frame = CGRect(x: size.width / 2, y: size.height / 2, width: size.width / 2, height: size.height / 2)
watermarkLayer.opacity = 0.5

let parentLayer = CALayer()
let videoLayer = CALayer()
parentLayer.frame = CGRect(x: 0, y: 0, width: size.width, height: size.height)
videoLayer.frame = CGRect(x: 0, y: 0, width: size.width, height: size.height)
parentLayer.addSublayer(videoLayer)
parentLayer.addSublayer(watermarkLayer)

let videoComposition = AVMutableVideoComposition()
videoComposition.animationTool = AVVideoCompositionCoreAnimationTool(postProcessingAsVideoLayer: videoLayer, in: parentLayer)
videoComposition.renderSize = size
videoComposition.frameDuration = CMTime(value: 1, timescale: 30)

// 4> 创建 AVAssetExportSession 来导出带有水印的视频
guard let exporter = AVAssetExportSession(asset: mixComposition, presetName: AVAssetExportPresetHighestQuality) else { return }
exporter.videoComposition = videoComposition
exporter.outputFileType = .mp4
let outputURL = // 导出路径
exporter.outputURL = outputURL
exporter.shouldOptimizeForNetworkUse = true

exporter.exportAsynchronously(completionHandler: {
    switch exporter.status {
    case .completed:
        print("Exported successfully")
    case .failed, .cancelled:
        print("Failed: \(exporter.error)")
    default:
        break
    }
})
```

:::

#### ❓`iOS` 中的倍速播放与导出

::: details 💡

  - 两倍速播放视频

    ```swift
    import AVFoundation
    
    // AVPlayer 的 rate 属性可以用来控制视频的播放速度
    let player = AVPlayer(url: url)
    player.rate = 2.0 // 2倍速度
    player.play()
    ```

  - 两倍速导出视频
    
    ```swift
    import AVFoundation
    
    func exportDoubleSpeedVideo(inputUrl: URL, outputUrl: URL, completion: @escaping (Bool, Error?) -> Void) {
        let asset = AVURLAsset(url: inputUrl)
        let composition = AVMutableComposition()
      
        // 处理视频轨
        if let videoTrack = asset.tracks(withMediaType: .video).first,
            let compositionVideoTrack = composition.addMutableTrack(withMediaType: .video,
                                                               preferredTrackID: kCMPersistentTrackID_Invalid) {
            try? compositionVideoTrack.insertTimeRange(videoTrack.timeRange, of: videoTrack, at: .zero)
            // 将 compositionTrack 的 preferredRate 设为 2.0，视频以两倍速播放
            compositionVideoTrack.preferredRate = 2.0
        }
        
        // 处理音频轨
        if let audioTrack = asset.tracks(withMediaType: .audio).first,
            let compositionAudioTrack = composition.addMutableTrack(withMediaType: .audio,
                                                               preferredTrackID: kCMPersistentTrackID_Invalid) {
            try? compositionAudioTrack.insertTimeRange(audioTrack.timeRange, of: audioTrack, at: .zero)
            // 音频以两倍速播放
            compositionAudioTrack.preferredRate = 2.0
        }
        
        // 导出两倍速视频
        let exportSession = AVAssetExportSession(asset: composition, presetName: AVAssetExportPresetHighestQuality)
        exportSession?.outputURL = outputUrl
        exportSession?.outputFileType = .mp4
        exportSession?.exportAsynchronously(completionHandler: {
            switch exportSession?.status {
            case .completed:
                completion(true, nil)
            case .cancelled, .failed:
                completion(false, exportSession?.error)
            default:
                completion(false, nil)
            }
        })
    }
    ```

:::

### 播放器

#### ❓缓存机制

::: details 💡



:::

#### ❓边播边下

::: details 💡



:::

#### ❓设计一个通用视频播放器

::: details 💡



:::
