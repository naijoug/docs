---
title: 音视频
icon: hashtag

index: true

---

<!-- more -->

### ❓`GPUImage` 框架介绍

::: details 💡

`GPUImage` 是开源的图像和视频处理框架，对视频处理、人像美颜、图像处理、滤镜处理等领域有广泛应用。其主要特点有：

1. 强大的功能：`GPUImage` 中内置了许多滤镜，用户可以对视频和图片进行各种各样的应用，也可以自己实现自定义的滤镜。

2. 高性能：`GPUImage` 是基于 OpenGL ES 构建的，能够充分利用设备的 GPU 处理大量计算任务，比使用 CPU 处理效率更高，对实时视频处理场景非常有利。

3. 易用性：`GPUImage` 的 API 设计封装得相当完善，使用起来十分方便，同时文档全面，上手容易。

4. 富有扩展性：`GPUImage` 也允许用户自定义滤镜，用户可以实现自己的设计，具有很高的灵活性。

5. 跨平台：`GPUImage` 既有适用于 iOS 的版本，也有适用于 Android 的版本，这大大方便了在多平台开发过程中的一致性。

注意，虽然 `GPUImage` 很强大，但由于使用了 OpenGL ES，所以在一些不支持或者 OpenGL ES 性能较弱的设备上可能会有一些问题，这是使用 `GPUImage` 需要注意的地方。

:::

### ❓`OpenGL` 处理流程

::: details 💡

OpenGL（Open Graphics Library）是一个用于渲染2D和3D图形的跨语言、跨平台的API。这种能力使得OpenGL成为虚拟现实、游戏、三维绘图等领域的标准规范。

OpenGL的处理过程通常分为以下几个阶段：

1. **顶点处理**：在这个阶段，你的原始顶点坐标会通过一系列的操作变换到屏幕坐标系。这些操作包括变换（旋转、缩放、平移）、裁剪（去除不在视野内的顶点）等。这个阶段是可以用自定义的顶点着色器（Vertex Shader）程序进行处理的。

2. **图元装配**：在所有的顶点被处理后，OpenGL开始将这些顶点装配成图形基元（如点、线、面）。

3. **光栅化**：图元在屏幕上被划分为一系列的片元（像素），这个过程就叫做光栅化。

4. **片元处理**：在这个阶段，片元着色器（Fragment Shader）会处理每个片元，比如对片元添加颜色、纹理或执行更复杂的效果。

5. **深度测试和混合**：OpenGL对这些片元进行深度测试，判断哪些片元是真正可见的。上面的所有阶段都只用来生成一堆片元，以及每个片元对应的深度值。最后如果开启混合，还会根据源与目标的alpha值来计算最后的颜色值。

值得注意的是，OpenGL是一个状态机，它会保存你设置的所有状态，直到你将其改变为止。因此，在绘制过程中，你需要处理你的OpenGL状态以防止你从前的设置影响到后面的渲染。

以上就是对OpenGL处理流程的简要介绍，具体实现会有更多细节可深入学习。

:::

### ❓`iOS` 中如何实现视频两倍速播放，并两倍速导出

::: details 💡

在iOS中，利用AVFoundation框架，可以轻松更改视频播放的速度。为了将视频导出为二倍速，你需要创建一个新的视频，并在每一帧时钟加倍。下面是详细的步骤：

1. 播放视频的两倍速：

AVPlayer的rate属性可以用来控制视频的播放速度。例如，下面的代码将会让视频Play的速度加倍：

```swift
let player = AVPlayer(url: url)
player.rate = 2.0 // 2倍速度
player.play()
```

2. 导出两倍速的视频：

这方面需要使用到AVAssetExportSession，另外以需要对视频的每一帧进行处理，将其时钟加倍。这需要依赖一些相对复杂的代码，你可以参考一些开源库（如：SDAVAssetExportSession）来实现。

导出带有速度调整的视频时，不仅需要修改视频轨道的时间，还可能需要调整音频轨道的播放速度以确保音视频同步。

另外，这个过程可能会非常消耗性能，所以通常需要在后台线程进行，避免阻塞主线程。

Ps. AVFoundation中的一些类在使用时需要特别注意其生命周期。需注意在适当的时机释放这些对象，以避免内存泄漏或者由于对象被过早释放导致的崩溃。

以上就是在iOS的AVFoundation框架中，实现视频两倍速播放和导出的大致步骤和需要注意的一些问题。如果有更多具体的问题可以再详细询问。

:::

### ❓`PCM` 计算时间

::: details 💡

PCM（Pulse Code Modulation）是一种数字化音频的方式。计算PCM数据的时长可以根据以下公式：

```
时间（秒）= 数据大小（字节） / (采样率 * 位深度/8 * 通道数)
```

其中：

- 数据大小即你的PCM文件的大小(字节）
- 采样率是指录音设备在一秒钟内对声音信号的采样次数，通常是44.1kHz，48kHz或96kHz。
- 位深度是指录音设备在对每次采样的声音信号进行模拟转数字的处理过程中，能够记录的声音信号的精度，通常是16位或24位。
- 通道数是音频的声道数，如单声道（Mono）为1，立体声（Stereo）为2。

假如你有一个5MB的PCM文件，采样率为44.1kHz，采样位深度为16位，为立体声音频，那么这段PCM数据的时长可以计算为：

```
时间 = 5 * 1024 * 1024 (字节) / (44.1 * 1024 * 16/8 * 2) ≈ 30秒
```
这样你就得到了这段音频的大概时长。

:::

### ❓`iOS` 中音频降燥如何处理

::: details 💡

在iOS平台，我们可以通过一些第三方库（例如 WebRTC、EZAudio）来进行降噪处理。但是，苹果自身并没有提供直接的音频降噪API。

1. WebRTC：这是由Google开发，用于音频和视频的实时通信。它拥有很好的语音处理模块，能够进行噪声抑制。这是一个开源库，你需要自行编译并添加到你的项目中。

2. EZAudio：这是一个iOS和macOS的音频框架，是对Core Audio API的简化封装，使得开发者更加简单地使用。虽然EZAudio本身不提供降噪功能，但是它便于你实时获取到音频的数据，接着你就可以对音频数据进行降噪处理。

3. `Superpowered`

:::

### ❓`iOS` 中视频合成如何处理

::: details 💡

> 使用 `AVFoundation` 框架中的 `AVMutableComposition` 类来合成多个视频片段

```swift
import AVFoundation

// 创建一个 AVMutableComposition 对象，它表示一个新的可变编辑环境：
let mixComposition = AVMutableComposition()
// 获取需要合成的视频资料并创建对应的 AVAsset 对象
let videoAssetA = AVAsset(url: URL(fileURLWithPath: "path/to/video_a.mp4"))
let videoAssetB = AVAsset(url: URL(fileURLWithPath: "path/to/video_b.mp4"))
// 创建 AVMutableCompositionTrack 对象并添加视频文件：
if let compositionVideoTrack = mixComposition.addMutableTrack(withMediaType: AVMediaType.video, preferredTrackID: CMPersistentTrackID(kCMPersistentTrackID_Invalid)),
    let sourceTrackA = videoAssetA.tracks(withMediaType: AVMediaType.video).first,
    let sourceTrackB = videoAssetB.tracks(withMediaType: AVMediaType.video).first {
        do {
            try compositionVideoTrack.insertTimeRange(CMTimeRangeMake(start: CMTime.zero, duration: videoAssetA.duration), of: sourceTrackA, at: CMTime.zero)
            try compositionVideoTrack.insertTimeRange(CMTimeRangeMake(start: CMTime.zero, duration: videoAssetB.duration), of: sourceTrackB, at: videoAssetA.duration)
        } catch {
            print(error)
        }
}
// 创建 AVAssetExportSession 对象，设置输出格式和路径，然后开始导出
let outputFilePath = "path/to/output.mp4"
let outputFileURL = URL(fileURLWithPath: outputFilePath)
if let exporter = AVAssetExportSession(asset: mixComposition, presetName: AVAssetExportPresetHighestQuality) {
    exporter.outputURL = outputFileURL
    exporter.outputFileType = AVFileType.mp4
    exporter.shouldOptimizeForNetworkUse = true
    exporter.exportAsynchronously {
        switch exporter.status {
        case .completed:
            print("Export complete")
        case .failed:
            print("Failed: \(exporter.error?.localizedDescription ?? "unknown error")")
        case .cancelled:
            print("Cancelled")
        default:
            print("Unknown")
        }
    }      
}
```

:::

------

## 编码

### ❓硬编码 vs 软编码

::: details 💡

硬编码和软编码是用于处理数字媒体编码/解码的两种基本方法。以下是他们的基本特点：

硬编码（硬件编码）：
- 硬编码是通过硬件来进行媒体数据的编码和解码。这些硬件包括用于编码解码（Codec）的专门芯片或者GPU等。
- 硬编码通常比软编码快，因为硬件编码器是专门设计用于编码/解码的，它们的算法优化级别比软件更高。
- 硬编码消耗的CPU资源较少，因为大部分工作都在特定的硬件上执行。
- 硬编码的缺点是灵活性差，依赖于硬件的能力，针对个别视频编码/解码技术可能没有等效的硬件支持。

软编码（软件编码）：
- 软编码是在CPU上进行编码和解码的过程。软编码不依赖于特定的硬件，而是依赖CPU和编码/解码软件（例如FFmpeg，x264等）。
- 软编码的优势是灵活性高，它可以很容易地优化和升级，以适应新的编码标准和格式。而且通常软编码在视频质量和压缩效率方面表现更好。
- 软编码的缺点是耗费的CPU资源较多，可能导致系统反应慢或者其他任务变慢。

总的来说，应选择硬编码和软编码需要根据具体的应用场景和需求决定。例如，如果是在电池供电的设备上进行实时视频流处理，可能更倾向于使用硬编码，因为它对电池使用的影响更小。然而，如果需要最高的视频质量和最小的文件大小，可能会选择软编码。

:::

### ❓FBO、H264

::: details 💡



:::

------

## 滤镜

### ❓滤镜实现原理

::: details 💡



:::

### ❓如何实现分割滤镜

::: details 💡



:::

## 直播

### ❓直播整体流程?

::: details 💡

直播的整体流程可以划分为以下几个主要步骤：

1. **信号采集**：使用手机、电脑或专用设备采集音视频信息。这个过程通常需要调用设备的摄像头和麦克风等硬件设备进行实时音视频录制。

2. **编码与压缩**：将采集到的音视频信号进行编码和压缩，转成可以网络传输的流媒体格式，如H.264, H.265, VP8, VP9等视频编码和AAC、MP3等音频编码。

3. **上传推流**：将编码后的音视频数据通过网络发送到直播服务器。这个过程需要稳定、连续的网络连接来保证信号的实时性。

4. **服务器接收与转发**：直播服务器接收到直播数据后，会进行一些必要的处理，如转码、录制、插播广告等，然后转发给观看的用户。

5. **观众拉流&解码**：观众端拉取直播流数据，然后解码播放。这个过程也需要稳定的网络环境，并且要解决视频播放的同步和时延问题。

6. **交互反馈**：观众与主播进行互动，如发弹幕、送礼物、评论等，这些都需要实时反馈到直播界面上。

这个过程需要遵循一定的协议和标准，如RTMP, HLS, WebRTC等流媒体传输协议，为了提高视频观看体验，还需要解决延迟、丢包、抖动等网络问题。而且在每一个步骤中都可能涉及到大量的技术细节和挑战。

:::

### ❓视频从`采集 -> 显示 -> 保存` 整个流程

::: details 💡

视频的整个工作流程通常可以划分为以下几个步骤：

1. **采集**：视频的采集通常由摄像头完成，摄像头将实时场景转换为数字信号。此过程涉及到图像采集设备的工作原理，以及光学和感光原理等内容。

2. **预处理**：采集到的数字信号进行预处理，这包括颜色空间转换，降噪处理，明暗处理等步骤，目的是提高视频的感知质量。

3. **编码**：编码阶段将预处理后的视频数据转换为特定格式，使其能够被存储或传输。编码通常需要考虑压缩的需要，用来减少视频数据的大小，包含H.264，H.265，VP8，VP9等编码格式。

4. **封装**：编码后的视频数据需要进行封装，封装数据包括音频流、视频流以及字幕等，常见的封装格式有MP4，FLV，MKV，AVI，MOV等。

5. **传输**：视频数据通过网络传输到服务器或者传输到其他设备。可以使用RTMP，HLS，HTTP-FLV等传输协议。

6. **解码**：接收端收到视频数据后，先解封装，然后将音频和视频数据解码回原始格式。

7. **后处理**：解码后的数据可以进行一定的后处理，如通过硬件加速提高播放性能，或者对图像进行增强处理等。

8. **显示**：将解码后的视频数据显示到屏幕上。

9. **保存**：如果需要，可以将视频数据保存到本地存储设备中。此时，视频数据通常以某种文件格式保存，可以是已经编码和封装的格式，也可以是原始的未编码格式。

以上就是视频从采集到显示再到保存的整个流程。每个步骤中都可能涉及到大量的技术细节和挑战。

:::

### ❓`rtmp`、`webrtc`

::: details 💡



:::

### ❓`ffmpeg`

### ❓`OpenCV`

------

## `AVFoundation`

### ❓`AVFoundation` & `GPUImage` & `OpenGL ES` & `MetalKit`

::: details 💡

:::

### ❓`samplebuffer` vs `pixelbuffer`

::: details 💡

  - `CMSampleBufferRef` : `Core Media`框架中被用来管理音频、视频、字幕等样本的集合。一个 `samplebuffer` 包含一到多个样本以及描述这些样本的元数据，例如显示时间戳、解码时间戳、持续时间、帧速率等。`samplebuffer`在更高的层级管理数据，包含了解压缩和播放媒体所需的所有信息。

  - `CVPixelBufferRef` : `Core Video` 框架中被用来管理图像数据。一个 `pixelbuffer` 表示一个视频帧，它包含了视频帧的原始像素数据。`pixelbuffer` 是在更低的层级处理数据，仅仅包含了像素信息，没有时间戳、持续时间等元数据。

  > 两者区别：`samplebuffer` 是对包含音频、视频、字幕等样本的集合的封装，包含了播放媒体所需的所有信息，`pixelbuffer` 是对视频帧的原始像素数据的封装，更关注图像层面的细节。

:::

### ❓`CMTime` 结构

::: details 💡

:::

### ❓如何使用 `AVFoundation` 给视频添加水印

::: details 💡

使用 `AVFoundation` 给视频添加水印主要涉及到以下几个步骤：

1. 创建 `AVMutableComposition` 对象，它用来管理所有的音频、视频和时间线资源，并给它添加视频轨道（`AVMutableCompositionTrack`

    ```swift
    let mixComposition = AVMutableComposition()
    guard let videoTrack = mixComposition.addMutableTrack(withMediaType: .video, preferredTrackID: Int32(kCMPersistentTrackID_Invalid)) else { return }
    ```

2. 从原始视频资源获取轨道，并添加到 composition 中。

   ```swift
    guard let assetTrack = asset.tracks(withMediaType: .video).first else { return }
    do {
        try videoTrack.insertTimeRange(CMTimeRangeMake(start: CMTime.zero, duration: asset.duration),
                                       of: assetTrack,
                                       at: CMTime.zero)
    } catch {
        print("Failed to load video track")
        return
    }
    ```

3. 创建一个 `CALayer` 用于显示水印并添加到 composition 的 layer 中。

    ```swift
    let size = videoTrack.naturalSize
    let watermarkLayer = CATextLayer()
    watermarkLayer.string = "Watermark"
    watermarkLayer.foregroundColor = UIColor.red.cgColor
    watermarkLayer.fontSize = 40
    watermarkLayer.frame = CGRect(x: size.width / 2, y: size.height / 2, width: size.width / 2, height: size.height / 2)
    watermarkLayer.opacity = 0.5

    let parentLayer = CALayer()
    let videoLayer = CALayer()
    parentLayer.frame = CGRect(x: 0, y: 0, width: size.width, height: size.height)
    videoLayer.frame = CGRect(x: 0, y: 0, width: size.width, height: size.height)
    parentLayer.addSublayer(videoLayer)
    parentLayer.addSublayer(watermarkLayer)

    let videoComposition = AVMutableVideoComposition()
    videoComposition.animationTool = AVVideoCompositionCoreAnimationTool(postProcessingAsVideoLayer: videoLayer, in: parentLayer)
    videoComposition.renderSize = size
    videoComposition.frameDuration = CMTime(value: 1, timescale: 30)
    ```

4. 创建 `AVAssetExportSession` 来导出带有水印的视频。

    ```swift
    guard let exporter = AVAssetExportSession(asset: mixComposition, presetName: AVAssetExportPresetHighestQuality) else { return }
    exporter.videoComposition = videoComposition
    exporter.outputFileType = .mp4
    let outputURL = // your output url
    exporter.outputURL = outputURL
    exporter.shouldOptimizeForNetworkUse = true

    exporter.exportAsynchronously(completionHandler: {
        switch exporter.status {
        case .completed:
            print("Exported successfully")
        case .failed, .cancelled:
            print("Failed: \(exporter.error)")
        default:
            break
        }
    })
    ```
    
以上这段代码给出了一个基本的在视频中添加文本水印的示例。如果你要添加图片水印，你可以创建一个 `CALayer`，向其中添加一张 `UIImage`。另外在真实场景下，你可能需要处理更多的错误和 edge cases。

:::

------

## AVPlayer

### ❓实现原理

::: details 💡



:::

### ❓缓存机制

::: details 💡



:::

### ❓边播边下

::: details 💡



:::

### ❓设计一个通用视频播放器

::: details 💡



:::
