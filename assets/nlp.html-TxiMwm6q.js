import{_ as l}from"./plugin-vue_export-helper-x3n3nnut.js";import{r,o as a,c as s,a as i,b as e,e as o,d as t,f as u}from"./app-9TEjBeIh.js";const h={},c=e("blockquote",null,[e("p",null,[e("code",null,"NLP"),o("("),e("code",null,"Natural Language Processing"),o(") : è‡ªç„¶è¯­è¨€å¤„ç†")])],-1),g=u('<blockquote><ul><li><strong>Wikipedia</strong>: è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆè‹±è¯­ï¼šNatural Language Processingï¼Œç¼©å†™ä½œ NLPï¼‰æ˜¯äººå·¥æ™ºæ…§å’Œè¯­è¨€å­¦é¢†åŸŸçš„åˆ†æ”¯å­¦ç§‘ã€‚æ­¤é¢†åŸŸæ¢è®¨å¦‚ä½•å¤„ç†åŠè¿ç”¨è‡ªç„¶è¯­è¨€ï¼›è‡ªç„¶è¯­è¨€å¤„ç†åŒ…æ‹¬å¤šæ–¹é¢å’Œæ­¥éª¤ï¼ŒåŸºæœ¬æœ‰è®¤çŸ¥ã€ç†è§£ã€ç”Ÿæˆç­‰éƒ¨åˆ†ã€‚</li><li><strong>Baidu</strong>: è‡ªç„¶è¯­è¨€å¤„ç†( Natural Language Processing, NLP)æ˜¯ä»¥è¯­è¨€ä¸ºå¯¹è±¡ï¼Œåˆ©ç”¨è®¡ç®—æœºæŠ€æœ¯æ¥åˆ†æã€ç†è§£å’Œå¤„ç†è‡ªç„¶è¯­è¨€çš„ä¸€é—¨å­¦ç§‘,å³æŠŠè®¡ç®—æœºä½œä¸ºè¯­è¨€ç ”ç©¶çš„å¼ºå¤§å·¥å…·ï¼Œåœ¨è®¡ç®—æœºçš„æ”¯æŒä¸‹å¯¹è¯­è¨€ä¿¡æ¯è¿›è¡Œå®šé‡åŒ–çš„ç ”ç©¶,å¹¶æä¾›å¯ä¾›äººä¸è®¡ç®—æœºä¹‹é—´èƒ½å…±åŒä½¿ç”¨çš„è¯­è¨€æå†™ã€‚åŒ…æ‹¬è‡ªç„¶è¯­è¨€ç†è§£( NaturalLanguage Understanding, NLU)å’Œè‡ªç„¶è¯­è¨€ç”Ÿæˆ( Natural LanguageGeneration, NLG)ä¸¤éƒ¨åˆ†ã€‚</li><li><strong>Google</strong>: ä½œä¸ºäººå·¥æ™ºèƒ½æŠ€æœ¯çš„åˆ†æ”¯ï¼ŒNLPï¼ˆè‡ªç„¶è¯­è¨€å¤„ç†ï¼‰ä½¿ç”¨æœºå™¨å­¦ä¹ æ¥å¤„ç†å’Œè§£é‡Šæ–‡æœ¬å’Œæ•°æ®ã€‚è‡ªç„¶è¯­è¨€è¯†åˆ«å’Œè‡ªç„¶è¯­è¨€ç”Ÿæˆæ˜¯ NLP çš„ç±»å‹ã€‚</li><li><strong>Amazon</strong>: è‡ªç„¶è¯­è¨€å¤„ç†(NLP) æ˜¯ä¸€ç§æœºå™¨å­¦ä¹ æŠ€æœ¯ï¼Œä½¿è®¡ç®—æœºèƒ½å¤Ÿè§£è¯»ã€å¤„ç†å’Œç†è§£äººç±»è¯­è¨€ã€‚</li><li><strong>Huawei</strong>: è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNatural Language Processingï¼Œç®€ç§°NLPï¼‰æ˜¯ä¸€æ¬¾åŸºäºäººå·¥æ™ºèƒ½æŠ€æœ¯ï¼Œé’ˆå¯¹å„ç±»ä¼ä¸šåŠå¼€å‘è€…æä¾›çš„ç”¨äºæ–‡æœ¬åˆ†æåŠæŒ–æ˜çš„äº‘æœåŠ¡ï¼Œæ—¨åœ¨å¸®åŠ©ç”¨æˆ·é«˜æ•ˆçš„å¤„ç†æ–‡æœ¬ã€‚</li><li><strong>Orace</strong>: è‡ªç„¶è¯­è¨€å¤„ç† (NLP) æ˜¯äººå·¥æ™ºèƒ½ (AI) çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒèƒ½å¤Ÿä½¿è®¡ç®—æœºç†è§£ã€ç”Ÿæˆå’Œå¤„ç†äººç±»è¯­è¨€ï¼Œæ”¯æŒç”¨æˆ·ä½¿ç”¨è‡ªç„¶è¯­è¨€æ–‡æœ¬æˆ–è¯­éŸ³æ¥è¯¢é—® (interrogate) æ•°æ®ï¼Œå› æ­¤åˆè¢«ç§°ä¸ºâ€œè¯­è¨€è¾“å…¥ (language in)â€ã€‚</li><li><strong>IBM</strong>: è‡ªç„¶è¯­è¨€å¤„ç†(NLP) æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œæ›´å…·ä½“åœ°è¯´ï¼Œæ˜¯äººå·¥æ™ºèƒ½(AI)çš„åˆ†æ”¯ï¼Œç›®æ ‡æ˜¯è®©è®¡ç®—æœºèƒ½å¤Ÿä»¥ä¸äººç±»å¤§è‡´ç›¸åŒçš„æ–¹å¼ç†è§£æ–‡æœ¬å’Œå£è¯­ã€‚</li><li><strong>Nvidia</strong>: è‡ªç„¶è¯­è¨€å¤„ç†(NLP) ä½¿ç”¨ AI å¤„ç†å’Œåˆ†ææ–‡æœ¬æˆ–è¯­éŸ³æ•°æ®ï¼Œä»¥ä¾¿ç†è§£å’Œè§£é‡Šå†…å®¹ã€å¯¹å†…å®¹è¿›è¡Œåˆ†ç±»å’Œ/æˆ–ä»å†…å®¹ä¸­è·å¾—è§è§£ã€‚</li></ul></blockquote><h2 id="reference" tabindex="-1"><a class="header-anchor" href="#reference" aria-hidden="true">#</a> reference</h2>',2),_={href:"https://github.com/explosion/spaCy",target:"_blank",rel:"noopener noreferrer"},p=e("blockquote",null,[e("p",null,"ğŸ’« Industrial-strength Natural Language Processing (NLP) in Python")],-1),d={href:"https://github.com/nltk/nltk",target:"_blank",rel:"noopener noreferrer"},b=e("blockquote",null,[e("p",null,"Natural Language Toolkit")],-1),f={href:"https://github.com/stanfordnlp/CoreNLP",target:"_blank",rel:"noopener noreferrer"},k=e("blockquote",null,[e("p",null,"A Java suite of core NLP tools.")],-1),m={href:"https://github.com/apache/opennlp",target:"_blank",rel:"noopener noreferrer"},L=e("blockquote",null,[e("p",null,"A machine learning based toolkit for the processing of natural language text.")],-1),N={href:"https://github.com/huggingface/tokenizers",target:"_blank",rel:"noopener noreferrer"},P=e("blockquote",null,[e("p",null,"ğŸ’¥ Fast State-of-the-Art Tokenizers optimized for Research and Production")],-1),y={href:"https://github.com/facebookresearch/fastText",target:"_blank",rel:"noopener noreferrer"},q=e("blockquote",null,[e("p",null,"Library for fast text representation and classification.")],-1),T={href:"https://github.com/aboSamoor/polyglot",target:"_blank",rel:"noopener noreferrer"},A=e("blockquote",null,[e("p",null,"Multilingual text (NLP) processing toolkit")],-1),x={href:"https://github.com/fxsjy/jieba",target:"_blank",rel:"noopener noreferrer"},C=e("blockquote",null,[e("p",null,"â€œç»“å·´â€ä¸­æ–‡åˆ†è¯ï¼šåšæœ€å¥½çš„ Python ä¸­æ–‡åˆ†è¯ç»„ä»¶")],-1),v={href:"https://github.com/baidu/lac",target:"_blank",rel:"noopener noreferrer"},R=e("blockquote",null,[e("p",null,"(Lexical Analysis of Chinese) ç™¾åº¦NLPï¼šåˆ†è¯ï¼Œè¯æ€§æ ‡æ³¨ï¼Œå‘½åå®ä½“è¯†åˆ«ï¼Œè¯é‡è¦æ€§")],-1),S={href:"https://github.com/konlpy/konlpy",target:"_blank",rel:"noopener noreferrer"},E=e("blockquote",null,[e("p",null,"Python package for Korean natural language processing.")],-1),M={href:"https://github.com/mocobeta/janome",target:"_blank",rel:"noopener noreferrer"},V=e("blockquote",null,[e("p",null,"Japanese morphological analysis engine written in pure Python")],-1),I={href:"https://github.com/CAMeL-Lab/camel_tools",target:"_blank",rel:"noopener noreferrer"},w=e("blockquote",null,[e("p",null,"A suite of Arabic natural language processing tools developed by the CAMeL Lab at New York University Abu Dhabi.")],-1),z={href:"https://github.com/PyThaiNLP/pythainlp",target:"_blank",rel:"noopener noreferrer"},B=e("blockquote",null,[e("p",null,"Thai Natural Language Processing in Python.")],-1),j={href:"https://github.com/undertheseanlp/underthesea",target:"_blank",rel:"noopener noreferrer"},G=e("blockquote",null,[e("p",null,"Underthesea - Vietnamese NLP Toolkit")],-1),H={href:"https://github.com/trungtv/pyvi",target:"_blank",rel:"noopener noreferrer"},J=e("blockquote",null,[e("p",null,"Python Vietnamese Core NLP Toolkit")],-1),K={href:"https://github.com/vngrs-ai/vnlp",target:"_blank",rel:"noopener noreferrer"},U=e("blockquote",null,[e("p",null,"State-of-the-art, lightweight NLP tools for Turkish language. Developed by VNGRS.")],-1),D={href:"https://github.com/huseinzol05/malaya",target:"_blank",rel:"noopener noreferrer"},O=e("blockquote",null,[e("p",null,"Natural Language Toolkit for bahasa Malaysia")],-1),F={href:"https://huggingface.co/w11wo/indonesian-roberta-base-posp-tagger",target:"_blank",rel:"noopener noreferrer"},W=e("blockquote",null,[e("p",null,"Indonesian RoBERTa Base POSP Tagger")],-1),Y=e("h2",{id:"embedding",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#embedding","aria-hidden":"true"},"#"),o(" embedding")],-1),Z={href:"https://github.com/facebookresearch/LASER",target:"_blank",rel:"noopener noreferrer"},Q=e("blockquote",null,[e("p",null,"Language-Agnostic SEntence Representations")],-1),X={href:"https://github.com/RaRe-Technologies/gensim",target:"_blank",rel:"noopener noreferrer"},$=e("blockquote",null,[e("p",null,"Topic Modelling for Humans")],-1),ee=e("h2",{id:"tutorial",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#tutorial","aria-hidden":"true"},"#"),o(" Tutorial")],-1),oe={href:"https://huggingface.co/learn/nlp-course/zh-CN",target:"_blank",rel:"noopener noreferrer"},ne={href:"https://github.com/keon/awesome-nlp/blob/master/README-ZH-TW.md",target:"_blank",rel:"noopener noreferrer"};function te(le,re){const n=r("ExternalLinkIcon");return a(),s("div",null,[c,i(" more "),g,e("ul",null,[e("li",null,[e("a",_,[o("spaCy"),t(n)]),p]),e("li",null,[e("a",d,[o("NLTK"),t(n)]),b]),e("li",null,[e("a",f,[o("Stanford CoreNLP"),t(n)]),k]),e("li",null,[e("a",m,[o("Apache OpenNLP"),t(n)]),L]),e("li",null,[e("a",N,[o("Tokenizers"),t(n)]),P]),e("li",null,[e("a",y,[o("fastText"),t(n)]),q]),e("li",null,[e("a",T,[o("polyglot"),t(n)]),A]),e("li",null,[e("a",x,[o("jieba"),t(n)]),o(" : Chinese "),C]),e("li",null,[e("a",v,[o("LAC"),t(n)]),o(" : Chinese "),R]),e("li",null,[e("a",S,[o("KoNLPy"),t(n)]),o(" : Korean "),E]),e("li",null,[e("a",M,[o("Janome"),t(n)]),o(" : Japanese "),V]),e("li",null,[e("a",I,[o("CAMeL Tools"),t(n)]),o(" : Arabic "),w]),e("li",null,[e("a",z,[o("PyThaiNLP"),t(n)]),o(" : Thai "),B]),e("li",null,[e("a",j,[o("underthesea"),t(n)]),G]),e("li",null,[e("a",H,[o("pyvi"),t(n)]),J]),e("li",null,[e("a",K,[o("VNLP"),t(n)]),U]),e("li",null,[e("a",D,[o("Malaya"),t(n)]),O]),e("li",null,[e("a",F,[o("indonesian-roberta-base-posp-tagger"),t(n)]),W])]),Y,e("ul",null,[e("li",null,[e("a",Z,[o("LASER"),t(n)]),Q]),e("li",null,[e("a",X,[o("gensim"),t(n)]),$])]),ee,e("ul",null,[e("li",null,[e("a",oe,[o("HuggingFace NLP Course"),t(n)])]),e("li",null,[e("a",ne,[o("ä»¤äººè®šå˜†çš„è‡ªç„¶èªè¨€è™•ç†"),t(n)])])])])}const ie=l(h,[["render",te],["__file","nlp.html.vue"]]);export{ie as default};
